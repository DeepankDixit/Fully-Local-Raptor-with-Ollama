# Fully-Local-Raptor-with-Ollama
RAPTOR based conversational AI both using Ollama llama2 and nomic-embed-text embeddings running fully locally

### Installing the Dependencies

1. Clone the repository to your local machine
2. Run `pip install -r requirements.txt` to install the required dependencies

## How to run it locally

1. Run the `fully_local_raptor` file from the project directly using `streamlit run fully_local_raptor.py`
2. This will launch the app in your web browser
3. You can load multiple PDFs and DOCX files and click on Process. 
4. Once the processing is complete, you can chat with the bot about uploaded documents.
